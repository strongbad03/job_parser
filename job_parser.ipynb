{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "nltk.download('genesis')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for jobs\n",
    "job_url = 'https://jobs.lever.co/pachama/34b58e75-af8f-4217-a5c0-21df58bed6cc'\n",
    "platform_stop_words = ['apply', 'job', 'home', 'pagejobs', 'powered', 'andor', 'along']\n",
    "job_stop_words = ['pachama', 'san', 'francisco', 'youll', 'pachamas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def lever_co_parser(url):\n",
    "    \"\"\"parser for jobs.lever.co\"\"\"\n",
    "    extracted_text = ''\n",
    "    r = requests.get(url)\n",
    "    text = r.text\n",
    "    soup = BeautifulSoup(text, 'html')\n",
    "    # TODO lever.co delivers multiple copies of same section, need better dedupe\n",
    "    for s in soup.select('div'):\n",
    "        extracted_text += s.get_text(separator=' ')\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def clean_text(text, filter_stops=True):\n",
    "    \"\"\"removes stop words, punctuation, and duplicate sections\"\"\"\n",
    "    # tokenize by sentence to dedupe sections\n",
    "    sents = list(set(sent_tokenize(text)))\n",
    "    sents_text = ' '.join(sents) # concatenate back to a string\n",
    "    \n",
    "    # remove punctuation\n",
    "    punctuation= '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    sents_text_no_punc = \" \"\n",
    "    for s in sents_text:\n",
    "        if s not in punctuation:\n",
    "            sents_text_no_punc = sents_text_no_punc + s\n",
    "            \n",
    "    # remove stop words if desired, not useful for ngrams\n",
    "    if filter_stops:\n",
    "        word_tokens = word_tokenize(sents_text_no_punc)\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        stop_words = stop_words + platform_stop_words + job_stop_words\n",
    "        filtered_tokens = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = word_tokenize(sents_text_no_punc)\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def analyze_tokens(word_token_list, n):\n",
    "    \"\"\"takes a bag of tokens and returns a data frame of the 20 most common \"\"\"\n",
    "    fdist = FreqDist(word_token_list)\n",
    "    print('Most common terms:')\n",
    "    print(fdist.most_common(n))\n",
    "\n",
    "    \n",
    "def analyze_ngrams(word_token_list, bi_tri='bi', top=10, n=5):\n",
    "    \"\"\"takes tokenized list without stopwords removed, outputs top x n-grams that appear\n",
    "    at least n times\"\"\"\n",
    "    if bi_tri == 'tri':\n",
    "        trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "        finder = TrigramCollocationFinder.from_words(word_token_list)\n",
    "        finder.apply_freq_filter(n)\n",
    "        print('\\nMost common trigrams:\\n',finder.nbest(trigram_measures.pmi, top))\n",
    "    else:\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(word_token_list)\n",
    "        finder.apply_freq_filter(n)\n",
    "        print('\\nMost common bigrams:\\n',finder.nbest(bigram_measures.pmi, top))\n",
    "\n",
    "        \n",
    "def main(job_url):\n",
    "    job_text = lever_co_parser(job_url)\n",
    "    clean_job_word_tokens = clean_text(job_text)\n",
    "    cleanish_job_word_tokens = clean_text(job_text, filter_stops=False)\n",
    "    analyze_tokens(clean_job_word_tokens, 20)\n",
    "    analyze_ngrams(cleanish_job_word_tokens)\n",
    "    analyze_ngrams(cleanish_job_word_tokens, bi_tri='tri', top=10, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      "[('product', 30), ('team', 23), ('roadmap', 19), ('data', 17), ('verify', 14), ('strong', 10), ('5+', 10), ('engineers', 10), ('complex', 10), ('products', 10), ('mission', 9), ('remote', 9), ('new', 9), ('track', 8), ('focus', 8), ('communication', 8), ('use', 8), ('across', 7), ('climate', 7), ('manager', 7)]\n",
      "\n",
      "Most common bigrams:\n",
      " [('ambiguous', 'environment'), ('analytical', 'thought'), ('andor', 'researchfocused'), ('application', 'towards'), ('deeply', 'analytical'), ('ego', 'crisp'), ('entrepreneurial', 'spirit'), ('first', 'principles'), ('help', 'address'), ('image', 'processing')]\n",
      "\n",
      "Most common trigrams:\n",
      " [('analytical', 'thought', 'process'), ('andor', 'researchfocused', 'tools'), ('application', 'towards', 'image'), ('deeply', 'analytical', 'thought'), ('first', 'principles', 'nearmastery'), ('low', 'ego', 'crisp'), ('notion', 'or', 'similar'), ('thought', 'process', 'driven'), ('towards', 'image', 'processing'), ('translates', 'into', 'being')]\n"
     ]
    }
   ],
   "source": [
    "main(job_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
